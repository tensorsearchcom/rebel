.. _quick_start:

###########
Quick Start
###########

This guide provides a high-level overview of the REBEL workflow.

1. Define Your Tests
====================

Create your test files using REBEL's ``@test_case`` decorator pattern. You can define the user prompt, expected outputs, and test configurations.

See the :ref:`defining_tests` guide for more details.

.. code-block:: python

   from rebel import test_case
   from rebel.models import Message, RoleEnum, TestGroup, RetryParams, MyCustomMetric

   @test_case(
       messages=[
           Message(role=RoleEnum.system, content="You are a helpful assistant."),
           Message(role=RoleEnum.user, content="Count the letter 'r' in this text.")
       ]
   )
   def test_counting_accuracy():
       yield TestGroup(
           retry_params=RetryParams(count=3, aggregation_strategy="mean"),
           metrics=[MyCustomMetric()]
       )

2. Run Your Benchmarks
======================

Execute your tests using the ``rebel run`` command from your terminal. You can specify the test directory, an output folder for results, and your API configuration.

.. code-block:: bash

   rebel run --test-dir tests/ --output-folder results/ --api-config model_config.json

For more advanced configurations, such as using a custom API client, you can pass additional arguments:

.. code-block:: bash

   rebel run --test-dir tests/ --output-folder results/ \
     --api-client-module my_module \
     --api-client-class MyAPIClient \
     --api-client-args '{"api_key": "your-key"}'

3. Analyze the Results
======================

REBEL generates a detailed JSON report in your specified output directory, organized by model and timestamp. This allows for easy comparison and historical analysis.

See the :ref:`results` guide for more information on the output format.

Next Steps
----------

For a complete, end-to-end implementation, check out our detailed walkthrough: :ref:`example_openrouter`.
